{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.** **Setup - Get Training Data**\n",
        "\n",
        "\n",
        "*   Mount Drive w/ MRI Dataset\n",
        "*   Perform Data Augmentation\n",
        "*   Donwsample Images to create HR-LR Pairs\n",
        "\n",
        "**NOTE:**\n",
        "No need to create HR-LR pairs and augment data more than once\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tbrO7T-Vvxmh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjOcrZD43aTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac005355-27ec-4a44-fe83-c218566a64bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check GPU status\n",
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "-a2PO2hupf5Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2d5266-3691-4a13-8efc-783d43890ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 11 13:25:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "\n",
        "# Define your directory paths\n",
        "hr_training_dir = '/content/drive/MyDrive/BrainTumorDataset/BraTS18_T1CE_training'\n",
        "hr_validation_dir = '/content/drive/MyDrive/BrainTumorDataset/BraTS19_T1CE_validation'\n",
        "hr_testing_dir = '/content/drive/MyDrive/BrainTumorDataset/BraTS20_T1CE_testing'\n",
        "\n",
        "hr_training_2Dslices = '/content/drive/MyDrive/BrainTumorDataset/hr_train_2D'\n",
        "hr_validation_2Dslices = '/content/drive/MyDrive/BrainTumorDataset/hr_valid_2D'\n",
        "hr_testing_2Dslices = '/content/drive/MyDrive/BrainTumorDataset/hr_test_2D'\n",
        "\n",
        "lr_training_2Dslices = '/content/drive/MyDrive/BrainTumorDataset/lr_train_2D'\n",
        "lr_validation_2Dslices = '/content/drive/MyDrive/BrainTumorDataset/lr_valid_2D'\n",
        "lr_testing_2Dslices = '/content/drive/MyDrive/BrainTumorDataset/lr_test_2D'\n"
      ],
      "metadata": {
        "id": "GCjiwuNUBQOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_middle_slice(nii_path, output_dir):\n",
        "    # Load the NIfTI file\n",
        "    nii_image = nib.load(nii_path)\n",
        "    data = nii_image.get_fdata()\n",
        "\n",
        "    # Calculate the middle slice index along the third axis (assuming axial slices)\n",
        "    middle_index = data.shape[2] // 2\n",
        "\n",
        "    # Extract the middle slice\n",
        "    middle_slice = data[:, :, middle_index]\n",
        "\n",
        "    # Normalize the slice for image representation\n",
        "    normalized_slice = ((middle_slice - np.min(middle_slice)) / (np.max(middle_slice) - np.min(middle_slice))) * 255.0\n",
        "    slice_image = Image.fromarray(normalized_slice.astype(np.uint8))\n",
        "\n",
        "    # Save the slice image\n",
        "    filename = os.path.basename(nii_path).replace('.nii', '.jpg')\n",
        "    slice_image.save(os.path.join(output_dir, filename))\n",
        "\n",
        "def process_nii_files(nii_dir, output_2d_dir):\n",
        "    for nii_file in os.listdir(nii_dir):\n",
        "        if nii_file.endswith('.nii') or nii_file.endswith('.nii.gz'):\n",
        "            nii_path = os.path.join(nii_dir, nii_file)\n",
        "            extract_middle_slice(nii_path, output_2d_dir)"
      ],
      "metadata": {
        "id": "0Bhh9Ov-DigV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function for each dataset\n",
        "process_nii_files(hr_training_dir, hr_training_2Dslices)\n",
        "process_nii_files(hr_validation_dir, hr_validation_2Dslices)\n",
        "process_nii_files(hr_testing_dir, hr_testing_2Dslices)"
      ],
      "metadata": {
        "id": "1z3GoTvDEh1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c411901-ac10-42c5-e56d-3d050797cecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-45139c793901>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call the function for each dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocess_nii_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr_training_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_training_2Dslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprocess_nii_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr_validation_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_validation_2Dslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprocess_nii_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr_testing_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_testing_2Dslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2b543739de14>\u001b[0m in \u001b[0;36mprocess_nii_files\u001b[0;34m(nii_dir, output_2d_dir)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_nii_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnii_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_2d_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mnii_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnii_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnii_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.nii'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnii_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnii_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnii_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnii_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/BrainTumorDataset/BraTS18_T1CE_training'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import random\n",
        "\n",
        "# Define Augmentation and Downsampling Functions\n",
        "\n",
        "def random_rotation(image):\n",
        "    # Randomly choose an angle for rotation\n",
        "    angles = [90, 180, 270]\n",
        "    angle = random.choice(angles)\n",
        "    return image.rotate(angle)\n",
        "\n",
        "def random_flip(image):\n",
        "    # Randomly choose axis for flipping\n",
        "    flips = [Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM]\n",
        "    mode = random.choice(flips)\n",
        "    return image.transpose(mode)\n",
        "\n",
        "def random_scale(image, min_scale=0.9, max_scale=1.1):\n",
        "    # Randomly choose a scale factor\n",
        "    scale_factor = random.uniform(min_scale, max_scale)\n",
        "    width, height = image.size\n",
        "    scaled_width = int(width * scale_factor)\n",
        "    scaled_height = int(height * scale_factor)\n",
        "    return image.resize((scaled_width, scaled_height), Image.BICUBIC)\n",
        "\n",
        "def random_brightness_contrast(image):\n",
        "    # Randomly adjust brightness and contrast\n",
        "    enhancer = ImageEnhance.Brightness(image)\n",
        "    image = enhancer.enhance(random.uniform(0.8, 1.2))  # Adjust brightness\n",
        "    enhancer = ImageEnhance.Contrast(image)\n",
        "    return enhancer.enhance(random.uniform(0.8, 1.2))  # Adjust contrast\n",
        "\n",
        "def downsample_image(image, scale_factor):\n",
        "    # Calculate new dimensions based on scale factor\n",
        "    width, height = image.size\n",
        "    new_width = int(width / scale_factor)\n",
        "    new_height = int(height / scale_factor)\n",
        "\n",
        "    # Resize down and up using bicubic interpolation\n",
        "    image_down = image.resize((new_width, new_height), Image.BICUBIC)\n",
        "    image_up = image_down.resize((width, height), Image.BICUBIC)\n",
        "\n",
        "    return np.array(image_up)\n",
        "\n",
        "def process_images(hr_input_dir, lr_output_dir, scale_factor):\n",
        "    for img_name in os.listdir(hr_input_dir):\n",
        "        if img_name.endswith('.jpg'):  # Assuming the slices are saved as '.png'\n",
        "            img_path = os.path.join(hr_input_dir, img_name)\n",
        "            img = Image.open(img_path)\n",
        "\n",
        "            # Convert image to RGB if it's not already in that mode\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "\n",
        "            # Apply a series of augmentations\n",
        "            augmented_images = [\n",
        "                random_rotation(img),\n",
        "                random_flip(img),\n",
        "                random_scale(img),\n",
        "                random_brightness_contrast(img)\n",
        "            ]\n",
        "\n",
        "            # Save the original and augmented images in LR form\n",
        "            for idx, aug_img in enumerate(augmented_images):\n",
        "                img_lr = downsample_image(aug_img, scale_factor)\n",
        "                aug_img_name = f\"LR_aug{idx}_{img_name}\"\n",
        "                img_lr = Image.fromarray(img_lr)\n",
        "                img_lr.save(os.path.join(lr_output_dir, aug_img_name))\n"
      ],
      "metadata": {
        "id": "gnWUxMRp3ln4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directories = [\n",
        "    (hr_training_2Dslices, lr_training_2Dslices),\n",
        "    (hr_validation_2Dslices, lr_validation_2Dslices),\n",
        "    (hr_testing_2Dslices, lr_testing_2Dslices)\n",
        "]\n",
        "\n",
        "for hr_dir, lr_dir in directories:\n",
        "    if not os.listdir(lr_dir):  # Checks if the directory is empty\n",
        "        process_images(hr_dir, lr_dir, scale_factor=2)\n",
        "    else:\n",
        "        print(f\"2D Slices already downsampled and saved in {lr_dir}!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0VNYehiLeuK",
        "outputId": "eb233bbc-3eec-40ff-814d-1bb3f0ab7858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2D Slices already downsampled and saved in /content/drive/MyDrive/BrainTumorDataset/lr_train_2D!\n",
            "2D Slices already downsampled and saved in /content/drive/MyDrive/BrainTumorDataset/lr_valid_2D!\n",
            "2D Slices already downsampled and saved in /content/drive/MyDrive/BrainTumorDataset/lr_test_2D!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#im = cv2.imread('/content/drive/MyDrive/BrainTumorDataset/lr_train_2D/LR_aug0_BraTS2018_HGG_Brats18_2013_11_1_Brats18_2013_11_1_t1ce.jpg')\n",
        "#print(im.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi_vYQqFSKMa",
        "outputId": "94712899-80b7-4b32-c7e2-7996db803350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 240, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.** **Prepare Data Loader**\n",
        "\n",
        "\n",
        "*   Create Dataset Class to load and preprocess images\n",
        "\n",
        "  * Resizing & Normalization\n",
        "\n",
        "  * Return Images as PyTorch Tensors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R9Z0TGAOwnWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MRIImageDataset(Dataset):\n",
        "    def __init__(self, hr_dir, lr_dir, transform=None, num_augments=4):\n",
        "        self.hr_dir = hr_dir\n",
        "        self.lr_dir = lr_dir\n",
        "        self.transform = transform\n",
        "        self.num_augments = num_augments\n",
        "        self.hr_images = [img for img in os.listdir(hr_dir) if img.endswith('.jpg') or img.endswith('.png')]\n",
        "        self.image_pairs = [(hr_img, [f'LR_aug{idx}_{hr_img}' for idx in range(num_augments)]) for hr_img in self.hr_images]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hr_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hr_img_name, lr_img_names = self.image_pairs[idx]\n",
        "        selected_lr_img_name = random.choice(lr_img_names)  # Randomly select one LR image\n",
        "\n",
        "        hr_img_path = os.path.join(self.hr_dir, hr_img_name)\n",
        "        lr_img_path = os.path.join(self.lr_dir, selected_lr_img_name)\n",
        "\n",
        "        hr_image = Image.open(hr_img_path).convert('RGB')\n",
        "        lr_image = Image.open(lr_img_path).convert('RGB')\n",
        "\n",
        "         # Transformations\n",
        "        target_size = (240, 240)\n",
        "\n",
        "        # Resize if larger\n",
        "        if hr_image.size[0] > target_size[0] or hr_image.size[1] > target_size[1]:\n",
        "            hr_image = T.functional.center_crop(hr_image, target_size)\n",
        "            lr_image = T.functional.center_crop(lr_image, target_size)\n",
        "\n",
        "        # Pad if smaller\n",
        "        if hr_image.size[0] < target_size[0] or hr_image.size[1] < target_size[1]:\n",
        "            padding = [0, 0, target_size[0] - hr_image.size[0], target_size[1] - hr_image.size[1]]  # left, top, right, bottom\n",
        "            hr_image = T.functional.pad(hr_image, padding)\n",
        "            lr_image = T.functional.pad(lr_image, padding)\n",
        "\n",
        "        # Ensure final size is consistent\n",
        "        hr_image = T.functional.resize(hr_image, target_size)\n",
        "        lr_image = T.functional.resize(lr_image, target_size)\n",
        "\n",
        "        if self.transform:\n",
        "            hr_image = self.transform(hr_image)\n",
        "            lr_image = self.transform(lr_image)\n",
        "\n",
        "        return {'hr': hr_image, 'lr': lr_image}\n",
        "\n",
        "# Define transformation for your dataset\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_dataset = MRIImageDataset(hr_dir=hr_training_2Dslices, lr_dir=lr_training_2Dslices, transform=transform)\n",
        "validate_dataset = MRIImageDataset(hr_dir=hr_validation_2Dslices, lr_dir=lr_validation_2Dslices, transform=transform)\n",
        "test_dataset = MRIImageDataset(hr_dir=hr_testing_2Dslices, lr_dir=lr_testing_2Dslices, transform=transform)"
      ],
      "metadata": {
        "id": "BW9ldUUL41dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.** **Build Network Architecture**\n",
        "\n",
        "\n",
        "*   Number of layers\n",
        "\n",
        "*   Types of layers\n",
        "  * Convolutional, Pooling, Upsampling, etc. and activation functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "HdywpXsH-vP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.relu(out)\n",
        "        out = torch.cat([x, out], 1)\n",
        "        return out\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(DenseLayer(in_channels + i * growth_rate, growth_rate))\n",
        "        self.dense_layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense_layers(x)\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TransitionLayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.pool(out)\n",
        "        return out\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        out += residual\n",
        "        return out\n",
        "\n",
        "class SuperResolutionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SuperResolutionCNN, self).__init__()\n",
        "        growth_rate = 32\n",
        "        num_dense_layers = 4\n",
        "\n",
        "        # Initial convolution layer\n",
        "        self.initial_conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Dense Blocks and Transition Layers\n",
        "        self.denseblock1 = DenseBlock(64, growth_rate, num_dense_layers)\n",
        "        self.transition1 = TransitionLayer(64 + growth_rate * num_dense_layers, 64)\n",
        "        self.denseblock2 = DenseBlock(64, growth_rate, num_dense_layers)\n",
        "        self.transition2 = TransitionLayer(64 + growth_rate * num_dense_layers, 64)\n",
        "\n",
        "        # Residual Blocks\n",
        "        self.resblock1 = ResidualBlock(64)\n",
        "        self.resblock2 = ResidualBlock(64)\n",
        "\n",
        "        # Upsampling Layer\n",
        "        self.upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Output convolution\n",
        "        self.output_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.initial_conv(x)\n",
        "        out = self.denseblock1(out)\n",
        "        out = self.transition1(out)\n",
        "        out = self.denseblock2(out)\n",
        "        out = self.transition2(out)\n",
        "        out = self.resblock1(out)\n",
        "        out = self.resblock2(out)\n",
        "        out = self.upsample(out)\n",
        "        out = self.output_conv(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the model\n",
        "\n",
        "# Run on GPU\n",
        "device = 'cuda'\n",
        "\n",
        "model = SuperResolutionCNN().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "3l28uIhE-usO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6596b752-bcf2-4257-b6ca-d6ff5b10fd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SuperResolutionCNN(\n",
            "  (initial_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (denseblock1): DenseBlock(\n",
            "    (dense_layers): Sequential(\n",
            "      (0): DenseLayer(\n",
            "        (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): DenseLayer(\n",
            "        (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): DenseLayer(\n",
            "        (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): DenseLayer(\n",
            "        (conv): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (transition1): TransitionLayer(\n",
            "    (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  )\n",
            "  (denseblock2): DenseBlock(\n",
            "    (dense_layers): Sequential(\n",
            "      (0): DenseLayer(\n",
            "        (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): DenseLayer(\n",
            "        (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): DenseLayer(\n",
            "        (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): DenseLayer(\n",
            "        (conv): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (transition2): TransitionLayer(\n",
            "    (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  )\n",
            "  (resblock1): ResidualBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (resblock2): ResidualBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (upsample): Upsample(scale_factor=4.0, mode='bilinear')\n",
            "  (output_conv): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "id": "2Ql4O5kf3DQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b6905e-eadd-420a-fbc5-6001538823ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lpips"
      ],
      "metadata": {
        "id": "f78lF5wJAgNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb9680f-6667-4c0f-e91f-f87d5c0b5b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m41.0/53.8 kB\u001b[0m \u001b[31m959.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m997.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.2.1->lpips) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Installing collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.utils import save_image\n",
        "import torchmetrics\n",
        "import lpips\n",
        "lpips_fn = lpips.LPIPS(net='alex').to(device)  # Using AlexNet\n",
        "\n",
        "# Initialize Data Loaders and Model Parameters\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "validation_loader = DataLoader(validate_dataset, batch_size=16, shuffle=True)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def lr_psnr_metric(lr_images, hr_images):\n",
        "    lr_psnr_metric = torchmetrics.PeakSignalNoiseRatio().to(device)\n",
        "    lr_psnr_metric.update(lr_images, hr_images)\n",
        "    return lr_psnr_metric.compute()\n",
        "\n",
        "def calculate_pixel_std_dev(images):\n",
        "    images = images.to(device)  # Ensure images are on the GPU\n",
        "    std_devs = [torch.std(image).item() for image in images]\n",
        "    return sum(std_devs) / len(std_devs)\n",
        "\n",
        "\n",
        "def lpips_metric(output, target):\n",
        "    output = output.to(device)\n",
        "    target = target.to(device)\n",
        "    return lpips_fn(output, target).mean()\n",
        "\n",
        "\n",
        "# Function to compute evaluation metrics\n",
        "def compute_metrics(model, loader):\n",
        "    model.eval()\n",
        "    psnr_metric = torchmetrics.PeakSignalNoiseRatio().to(device)\n",
        "    ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure().to(device)\n",
        "    lpips_total, lr_psnr_total, pixel_std_dev_total = 0.0, 0.0, 0.0  # Initialize all accumulators\n",
        "\n",
        "    # Add more metrics as needed\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            hr_images = batch['hr'].to(device)\n",
        "            lr_images = batch['lr'].to(device)\n",
        "            outputs = model(lr_images)\n",
        "\n",
        "            psnr_metric.update(outputs, hr_images)\n",
        "            ssim_metric.update(outputs, hr_images)\n",
        "\n",
        "            # LPIPS calculation\n",
        "            lpips_value = lpips_fn(outputs, hr_images).mean()\n",
        "            lpips_total += lpips_value\n",
        "\n",
        "            # LR_PSNR calculation\n",
        "            lr_psnr_value = lr_psnr_metric(lr_images, hr_images)\n",
        "            lr_psnr_total += lr_psnr_value\n",
        "\n",
        "            # Pixel Standard Deviation calculation\n",
        "            pixel_std_dev_value = calculate_pixel_std_dev(outputs)\n",
        "            pixel_std_dev_total += pixel_std_dev_value\n",
        "\n",
        "\n",
        "    num_batches = len(loader)\n",
        "\n",
        "    return psnr_metric.compute(), ssim_metric.compute(), lpips_total / num_batches, lr_psnr_total / num_batches, pixel_std_dev_total / num_batches\n",
        "\n",
        "# Function to validate model\n",
        "def validate_model(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            hr_images = batch['hr'].to(device)\n",
        "            lr_images = batch['lr'].to(device)\n",
        "            outputs = model(lr_images)\n",
        "            loss = criterion(outputs, hr_images)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    return avg_loss\n",
        "\n",
        "def save_hr_images(images, directory, epoch, batch_idx):\n",
        "    os.makedirs(directory, exist_ok=True)  # Create directory if it doesn't exist\n",
        "    for i, image in enumerate(images):\n",
        "        save_path = os.path.join(directory, f\"epoch{epoch}_batch{batch_idx}_img{i}.jpg\")\n",
        "        save_image(image, save_path)"
      ],
      "metadata": {
        "id": "kHTFALY3qKKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1250f646-3116-486a-b4e5-dbf6f5a37e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:05<00:00, 41.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Loop\n",
        "best_loss = float('inf')\n",
        "patience = 5\n",
        "trigger_times = 0\n",
        "num_epochs = 30\n",
        "save_interval = 10\n",
        "generated_hr_dir = '/content/drive/MyDrive/BrainTumorDataset/generated_hr'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        hr_images = batch['hr'].to(device)\n",
        "        lr_images = batch['lr'].to(device)\n",
        "\n",
        "        outputs = model(lr_images)\n",
        "\n",
        "        # Save HR images conditionally\n",
        "        if epoch % save_interval == 0:  # 'save_interval' can be defined as per your requirement\n",
        "            save_hr_images(outputs.cpu(), generated_hr_dir, epoch, batch_idx)\n",
        "\n",
        "        loss = criterion(outputs, hr_images)\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_training_loss = total_loss / num_batches\n",
        "    val_loss = validate_model(model, validation_loader)\n",
        "    psnr, ssim, lpips_value, lr_psnr, pixel_std_dev = compute_metrics(model, validation_loader)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Training Loss: {avg_training_loss}, Validation Loss: {val_loss}, PSNR: {psnr}, SSIM: {ssim},lpips valu: {lpips_value}, LRPSNR: {lr_psnr} PXL STD: {pixel_std_dev}')\n",
        "\n",
        "    # # Early stopping logic\n",
        "    # if val_loss < best_loss:\n",
        "    #     best_loss = val_loss\n",
        "    #     trigger_times = 0\n",
        "    # else:\n",
        "    #     trigger_times += 1\n",
        "    #     if trigger_times >= patience:\n",
        "    #         print(\"Early stopping!\")\n",
        "    #         break\n",
        "\n"
      ],
      "metadata": {
        "id": "fYhTvNW-sTis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1492468d-12f4-4e38-daba-4ff74e402d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
            "  _future_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
            "  _future_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Avg Training Loss: 1.371220207048787, Validation Loss: 0.4206377693584987, PSNR: 17.302001953125, SSIM: 0.7063705325126648,lpips valu: 0.5252938866615295, LRPSNR: 22.128170013427734 PXL STD: 0.4652189782624241\n",
            "Epoch [2/30], Avg Training Loss: 0.25427332603269154, Validation Loss: 0.17722030409744807, PSNR: 21.24116325378418, SSIM: 0.7500584125518799,lpips valu: 0.41490232944488525, LRPSNR: 22.469463348388672 PXL STD: 0.6245036619480978\n",
            "Epoch [3/30], Avg Training Loss: 0.1672774205605189, Validation Loss: 0.1500229211080642, PSNR: 21.74248504638672, SSIM: 0.7599294781684875,lpips valu: 0.40570881962776184, LRPSNR: 22.426902770996094 PXL STD: 0.6986367851875874\n",
            "Epoch [4/30], Avg Training Loss: 0.16741596617632443, Validation Loss: 0.14681137956324078, PSNR: 21.62818145751953, SSIM: 0.7564318776130676,lpips valu: 0.3929511606693268, LRPSNR: 22.037668228149414 PXL STD: 0.6445175437066628\n",
            "Epoch [5/30], Avg Training Loss: 0.16260434314608574, Validation Loss: 0.14923226159243358, PSNR: 21.993976593017578, SSIM: 0.7586174011230469,lpips valu: 0.3767571747303009, LRPSNR: 21.995275497436523 PXL STD: 0.6826679614074782\n",
            "Epoch [6/30], Avg Training Loss: 0.13433686064349282, Validation Loss: 0.1492540194165139, PSNR: 22.440025329589844, SSIM: 0.7728875279426575,lpips valu: 0.35880061984062195, LRPSNR: 22.47950553894043 PXL STD: 0.6776205966008358\n",
            "Epoch [7/30], Avg Training Loss: 0.1465817793375916, Validation Loss: 0.11963684005396706, PSNR: 22.259628295898438, SSIM: 0.760466456413269,lpips valu: 0.353629469871521, LRPSNR: 21.952451705932617 PXL STD: 0.6702350261209368\n",
            "Epoch [8/30], Avg Training Loss: 0.14001809805631638, Validation Loss: 0.12781104977641786, PSNR: 22.27528953552246, SSIM: 0.7609093189239502,lpips valu: 0.34043368697166443, LRPSNR: 21.94719123840332 PXL STD: 0.6872379545021017\n",
            "Epoch [9/30], Avg Training Loss: 0.12515093717310163, Validation Loss: 0.1252302843190375, PSNR: 22.2474365234375, SSIM: 0.7676645517349243,lpips valu: 0.3344831168651581, LRPSNR: 21.874584197998047 PXL STD: 0.7142597574005727\n",
            "Epoch [10/30], Avg Training Loss: 0.13615592569112778, Validation Loss: 0.1184985822155362, PSNR: 22.909366607666016, SSIM: 0.7609788179397583,lpips valu: 0.3256095051765442, LRPSNR: 22.589954376220703 PXL STD: 0.6608673770006012\n",
            "Epoch [11/30], Avg Training Loss: 0.12840561320384344, Validation Loss: 0.11678417665617806, PSNR: 22.600601196289062, SSIM: 0.7656241059303284,lpips valu: 0.3228186070919037, LRPSNR: 22.371553421020508 PXL STD: 0.684513092585871\n",
            "Epoch [12/30], Avg Training Loss: 0.11333084478974342, Validation Loss: 0.13390299252101354, PSNR: 22.5805606842041, SSIM: 0.7686695456504822,lpips valu: 0.32376155257225037, LRPSNR: 22.22677230834961 PXL STD: 0.6937813889868811\n",
            "Epoch [13/30], Avg Training Loss: 0.1400396049850517, Validation Loss: 0.12672311528807595, PSNR: 22.453022003173828, SSIM: 0.7674063444137573,lpips valu: 0.33059045672416687, LRPSNR: 22.044736862182617 PXL STD: 0.6912575714214115\n",
            "Epoch [14/30], Avg Training Loss: 0.13842768842975298, Validation Loss: 0.11720090891633715, PSNR: 22.606136322021484, SSIM: 0.7577762007713318,lpips valu: 0.32318687438964844, LRPSNR: 22.316612243652344 PXL STD: 0.6643546425363644\n",
            "Epoch [15/30], Avg Training Loss: 0.1253529294497437, Validation Loss: 0.12421369942880812, PSNR: 22.991849899291992, SSIM: 0.7755770087242126,lpips valu: 0.3163791298866272, LRPSNR: 22.681821823120117 PXL STD: 0.7015273876006708\n",
            "Epoch [16/30], Avg Training Loss: 0.13140731698109043, Validation Loss: 0.1231277652439617, PSNR: 22.486547470092773, SSIM: 0.7661755681037903,lpips valu: 0.32576984167099, LRPSNR: 21.973892211914062 PXL STD: 0.6867005174764159\n",
            "Epoch [17/30], Avg Training Loss: 0.13630613394909435, Validation Loss: 0.11936042315903164, PSNR: 22.59678077697754, SSIM: 0.763274610042572,lpips valu: 0.316153347492218, LRPSNR: 22.10094451904297 PXL STD: 0.6962795654045684\n",
            "Epoch [18/30], Avg Training Loss: 0.1338130570948124, Validation Loss: 0.11126660165332612, PSNR: 22.681495666503906, SSIM: 0.7646093964576721,lpips valu: 0.31514331698417664, LRPSNR: 22.242116928100586 PXL STD: 0.6769824544107225\n",
            "Epoch [19/30], Avg Training Loss: 0.1221632348994414, Validation Loss: 0.10910748193661372, PSNR: 22.89932632446289, SSIM: 0.7742201685905457,lpips valu: 0.3116566240787506, LRPSNR: 22.394515991210938 PXL STD: 0.6855920465903825\n",
            "Epoch [20/30], Avg Training Loss: 0.12364758840865558, Validation Loss: 0.1317783643802007, PSNR: 22.713905334472656, SSIM: 0.7762605547904968,lpips valu: 0.31811097264289856, LRPSNR: 22.338857650756836 PXL STD: 0.7116567060655478\n",
            "Epoch [21/30], Avg Training Loss: 0.13291253770391145, Validation Loss: 0.1264749133870715, PSNR: 22.791349411010742, SSIM: 0.7700251936912537,lpips valu: 0.31773829460144043, LRPSNR: 22.06576919555664 PXL STD: 0.6809032520494697\n",
            "Epoch [22/30], Avg Training Loss: 0.12167991076906522, Validation Loss: 0.12320689892485029, PSNR: 22.327634811401367, SSIM: 0.7686036825180054,lpips valu: 0.3193696439266205, LRPSNR: 21.717300415039062 PXL STD: 0.675967094836896\n",
            "Epoch [23/30], Avg Training Loss: 0.12678797791401544, Validation Loss: 0.12530030700422468, PSNR: 23.023649215698242, SSIM: 0.7699565887451172,lpips valu: 0.31336304545402527, LRPSNR: 22.46733283996582 PXL STD: 0.684025188464494\n",
            "Epoch [24/30], Avg Training Loss: 0.11925177400310834, Validation Loss: 0.12319385366780418, PSNR: 22.7327880859375, SSIM: 0.7723674774169922,lpips valu: 0.3107379674911499, LRPSNR: 22.246482849121094 PXL STD: 0.7072591412761787\n",
            "Epoch [25/30], Avg Training Loss: 0.11238428722653124, Validation Loss: 0.11077736566464107, PSNR: 22.931949615478516, SSIM: 0.7753183245658875,lpips valu: 0.3070392906665802, LRPSNR: 22.41282844543457 PXL STD: 0.699065375173477\n",
            "Epoch [26/30], Avg Training Loss: 0.12125470654831992, Validation Loss: 0.11331736119020552, PSNR: 22.295001983642578, SSIM: 0.7665109038352966,lpips valu: 0.31165310740470886, LRPSNR: 21.453840255737305 PXL STD: 0.6814588517026634\n",
            "Epoch [27/30], Avg Training Loss: 0.12342874540223016, Validation Loss: 0.12142865040472575, PSNR: 23.016237258911133, SSIM: 0.7852848172187805,lpips valu: 0.3060649335384369, LRPSNR: 22.58134651184082 PXL STD: 0.6891836376265198\n",
            "Epoch [28/30], Avg Training Loss: 0.12499470719032818, Validation Loss: 0.11147481983616239, PSNR: 22.92518424987793, SSIM: 0.7713305950164795,lpips valu: 0.3078363537788391, LRPSNR: 22.640514373779297 PXL STD: 0.6778116659419675\n",
            "Epoch [29/30], Avg Training Loss: 0.12254577378431956, Validation Loss: 0.11448361156951814, PSNR: 22.849082946777344, SSIM: 0.7790834307670593,lpips valu: 0.3063073754310608, LRPSNR: 22.353443145751953 PXL STD: 0.7002385476996907\n",
            "Epoch [30/30], Avg Training Loss: 0.1332167349755764, Validation Loss: 0.12431505357935316, PSNR: 22.479551315307617, SSIM: 0.7593691945075989,lpips valu: 0.30949828028678894, LRPSNR: 22.042367935180664 PXL STD: 0.6553806254804945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFwP6a6y2eSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}